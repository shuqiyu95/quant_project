"""
Mag7 äº”æ—¥è½®åŠ¨ç­–ç•¥ - ä¸»è¿è¡Œè„šæœ¬

å®ç°å®Œæ•´çš„æµç¨‹ï¼š
1. æ•°æ®è·å–ï¼ˆMag7 è¿‡å»ä¸€å¹´æ•°æ®ï¼‰
2. ç‰¹å¾å·¥ç¨‹ï¼ˆåŸºç¡€é‡ä»·å› å­ï¼‰
3. æ¨¡å‹è®­ç»ƒï¼ˆéšæœºæ£®æ—/çº¿æ€§æ¨¡å‹ï¼‰
4. å›æµ‹ï¼ˆæ¯å‘¨ä¸€è°ƒä»“ï¼‰
5. æ€§èƒ½åˆ†æ

Usage:
    python main_mag7_strategy.py [--model_type random_forest] [--loss_type rank_mse]
"""

import os
import sys
import argparse
import pickle
from datetime import datetime, timedelta
from typing import Optional
import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from src.data_engine import DataManager
from src.models import FeatureEngineer, StockPredictor
from src.backtester import BacktestEngine, WeeklyRotationStrategy, PerformanceAnalyzer
from src.backtester.strategy import run_backtest_with_strategy


# Mag7 è‚¡ç¥¨åˆ—è¡¨
MAG7_SYMBOLS = ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'NVDA', 'META', 'TSLA']


def fetch_mag7_data(
    data_dir: str = "data",
    start_date: Optional[str] = None,
    end_date: Optional[str] = None,
    years: int = 1
) -> dict:
    """
    è·å– Mag7 æ•°æ®
    
    Args:
        data_dir: æ•°æ®ç›®å½•
        start_date: å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨è®¡ç®—
        end_date: ç»“æŸæ—¥æœŸ (YYYY-MM-DD)ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨å½“å‰æ—¥æœŸ
        years: è·å–å¤šå°‘å¹´çš„æ•°æ® (å½“start_dateä¸ºNoneæ—¶ä½¿ç”¨)
        
    Returns:
        data_dict: {symbol: df}
    """
    print("=" * 60)
    print("ğŸ“Š STEP 1: Fetching Mag7 Data")
    print("=" * 60)
    
    dm = DataManager(data_dir=data_dir)
    
    # è®¡ç®—æ—¥æœŸèŒƒå›´
    if end_date is None:
        end_dt = datetime.now()
    else:
        end_dt = datetime.strptime(end_date, '%Y-%m-%d')
    
    if start_date is None:
        start_dt = end_dt - timedelta(days=365 * years)
    else:
        start_dt = datetime.strptime(start_date, '%Y-%m-%d')
    
    # è½¬æ¢ä¸ºå­—ç¬¦ä¸²æ ¼å¼
    start_date_str = start_dt.strftime('%Y-%m-%d')
    end_date_str = end_dt.strftime('%Y-%m-%d')
    
    print(f"\nFetching data from {start_dt.date()} to {end_dt.date()}")
    print(f"Symbols: {', '.join(MAG7_SYMBOLS)}\n")
    
    data_dict = {}
    
    for symbol in MAG7_SYMBOLS:
        try:
            df = dm.fetch_data(
                symbol=symbol,
                start_date=start_date_str,
                end_date=end_date_str,
                use_cache=True
            )
            
            if df is not None and len(df) > 0:
                data_dict[symbol] = df
                print(f"âœ… {symbol}: {len(df)} days")
            else:
                print(f"âŒ {symbol}: No data")
        except Exception as e:
            print(f"âŒ {symbol}: Error - {e}")
    
    print(f"\nâœ… Successfully fetched {len(data_dict)}/{len(MAG7_SYMBOLS)} stocks")
    
    return data_dict


def prepare_training_data(
    data_dict: dict,
    forward_days: int = 5,
    test_ratio: float = 0.3
) -> tuple:
    """
    å‡†å¤‡è®­ç»ƒæ•°æ®
    
    Args:
        data_dict: {symbol: df}
        forward_days: é¢„æµ‹æœªæ¥å‡ å¤©
        test_ratio: æµ‹è¯•é›†æ¯”ä¾‹
        
    Returns:
        train_data, test_data, feature_engineer
    """
    print("\n" + "=" * 60)
    print("ğŸ”§ STEP 2: Feature Engineering")
    print("=" * 60)
    
    # åˆ›å»ºç‰¹å¾å·¥ç¨‹å™¨
    fe = FeatureEngineer()
    
    # å‡†å¤‡æ•°æ®é›†
    print("\nGenerating features and labels...")
    X, y, dates, symbols = fe.prepare_dataset(
        data_dict,
        forward_days=forward_days,
        min_periods=60
    )
    
    print(f"âœ… Dataset prepared:")
    print(f"   Features shape: {X.shape}")
    print(f"   Labels shape: {y.shape}")
    print(f"   Date range: {min(dates).date()} to {max(dates).date()}")
    print(f"   Unique stocks: {len(set(symbols))}")
    
    # åˆ’åˆ†è®­ç»ƒæµ‹è¯•é›†ï¼ˆæŒ‰æ—¶é—´ï¼‰
    n_samples = len(X)
    split_idx = int(n_samples * (1 - test_ratio))
    
    X_train = X.iloc[:split_idx]
    X_test = X.iloc[split_idx:]
    y_train = y.iloc[:split_idx]
    y_test = y.iloc[split_idx:]
    dates_train = dates[:split_idx]
    dates_test = dates[split_idx:]
    
    print(f"\nTrain/Test Split:")
    print(f"   Training: {len(X_train)} samples ({dates_train[0].date()} to {dates_train[-1].date()})")
    print(f"   Testing:  {len(X_test)} samples ({dates_test[0].date()} to {dates_test[-1].date()})")
    
    train_data = (X_train, y_train, dates_train)
    test_data = (X_test, y_test, dates_test)
    
    return train_data, test_data, fe


def save_dataset(
    train_data: tuple,
    test_data: tuple,
    feature_engineer: FeatureEngineer,
    save_path: str
):
    """
    ä¿å­˜å¤„ç†å¥½çš„æ•°æ®é›†
    
    Args:
        train_data: (X_train, y_train, dates_train)
        test_data: (X_test, y_test, dates_test)
        feature_engineer: ç‰¹å¾å·¥ç¨‹å™¨
        save_path: ä¿å­˜è·¯å¾„ï¼ˆ.pklæ–‡ä»¶ï¼‰
    """
    print("\n" + "=" * 60)
    print("ğŸ’¾ Saving Dataset")
    print("=" * 60)
    
    X_train, y_train, dates_train = train_data
    X_test, y_test, dates_test = test_data
    
    # æ‰“åŒ…æ•°æ®
    dataset = {
        'X_train': X_train,
        'y_train': y_train,
        'dates_train': dates_train,
        'X_test': X_test,
        'y_test': y_test,
        'dates_test': dates_test,
        'feature_engineer': feature_engineer,
        'metadata': {
            'train_samples': len(X_train),
            'test_samples': len(X_test),
            'features': X_train.shape[1],
            'train_date_range': (min(dates_train).date(), max(dates_train).date()),
            'test_date_range': (min(dates_test).date(), max(dates_test).date()),
            'saved_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }
    }
    
    # åˆ›å»ºç›®å½•
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    
    # ä¿å­˜
    with open(save_path, 'wb') as f:
        pickle.dump(dataset, f)
    
    print(f"\nâœ… Dataset saved to: {save_path}")
    print(f"   Training samples: {dataset['metadata']['train_samples']}")
    print(f"   Testing samples: {dataset['metadata']['test_samples']}")
    print(f"   Features: {dataset['metadata']['features']}")
    print(f"   Train dates: {dataset['metadata']['train_date_range'][0]} to {dataset['metadata']['train_date_range'][1]}")
    print(f"   Test dates: {dataset['metadata']['test_date_range'][0]} to {dataset['metadata']['test_date_range'][1]}")


def load_dataset(load_path: str) -> tuple:
    """
    åŠ è½½å¤„ç†å¥½çš„æ•°æ®é›†
    
    Args:
        load_path: æ•°æ®é›†è·¯å¾„ï¼ˆ.pklæ–‡ä»¶ï¼‰
        
    Returns:
        train_data, test_data, feature_engineer
    """
    print("\n" + "=" * 60)
    print("ğŸ“‚ Loading Dataset")
    print("=" * 60)
    
    if not os.path.exists(load_path):
        raise FileNotFoundError(f"Dataset file not found: {load_path}")
    
    # åŠ è½½
    with open(load_path, 'rb') as f:
        dataset = pickle.load(f)
    
    print(f"\nâœ… Dataset loaded from: {load_path}")
    print(f"   Saved at: {dataset['metadata']['saved_at']}")
    print(f"   Training samples: {dataset['metadata']['train_samples']}")
    print(f"   Testing samples: {dataset['metadata']['test_samples']}")
    print(f"   Features: {dataset['metadata']['features']}")
    print(f"   Train dates: {dataset['metadata']['train_date_range'][0]} to {dataset['metadata']['train_date_range'][1]}")
    print(f"   Test dates: {dataset['metadata']['test_date_range'][0]} to {dataset['metadata']['test_date_range'][1]}")
    
    train_data = (dataset['X_train'], dataset['y_train'], dataset['dates_train'])
    test_data = (dataset['X_test'], dataset['y_test'], dataset['dates_test'])
    feature_engineer = dataset['feature_engineer']
    
    return train_data, test_data, feature_engineer


def train_model(
    train_data: tuple,
    test_data: tuple,
    model_type: str = 'random_forest',
    loss_type: str = 'rank_mse',
    save_path: Optional[str] = None
) -> StockPredictor:
    """
    è®­ç»ƒé¢„æµ‹æ¨¡å‹
    
    Args:
        train_data: (X_train, y_train, dates_train)
        test_data: (X_test, y_test, dates_test)
        model_type: æ¨¡å‹ç±»å‹
        loss_type: æŸå¤±å‡½æ•°ç±»å‹
        save_path: æ¨¡å‹ä¿å­˜è·¯å¾„
        
    Returns:
        predictor: è®­ç»ƒå¥½çš„æ¨¡å‹
    """
    print("\n" + "=" * 60)
    print("ğŸ¤– STEP 3: Training Model")
    print("=" * 60)
    
    X_train, y_train, _ = train_data
    X_test, y_test, _ = test_data
    
    print(f"\nModel Type: {model_type}")
    print(f"Loss Type: {loss_type}")
    
    # åˆ›å»ºå¹¶è®­ç»ƒæ¨¡å‹
    predictor = StockPredictor(
        model_type=model_type,
        loss_type=loss_type,
        scale_features=True
    )
    
    print("\nTraining...")
    predictor.fit(X_train, y_train)
    print("âœ… Training completed!")
    
    # è¯„ä¼°æ¨¡å‹
    print("\n" + "-" * 60)
    print("Model Evaluation")
    print("-" * 60)
    
    # è®­ç»ƒé›†è¯„ä¼°
    train_metrics = predictor.evaluate(X_train, y_train, k=3)
    print("\nğŸ“Š Training Set:")
    for key, value in train_metrics.items():
        print(f"   {key}: {value:.4f}")
    
    # æµ‹è¯•é›†è¯„ä¼°
    test_metrics = predictor.evaluate(X_test, y_test, k=3)
    print("\nğŸ“Š Test Set:")
    for key, value in test_metrics.items():
        print(f"   {key}: {value:.4f}")
    
    # ç‰¹å¾é‡è¦æ€§
    if hasattr(predictor.model, 'feature_importances_'):
        print("\nğŸ“ˆ Top 10 Important Features:")
        importance_df = predictor.get_feature_importance(top_k=10)
        for idx, row in importance_df.iterrows():
            print(f"   {row['feature']:.<30} {row['importance']:.4f}")
    
    # ä¿å­˜æ¨¡å‹
    if save_path:
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        predictor.save(save_path)
        print(f"\nğŸ’¾ Model saved to {save_path}")
    
    return predictor


def run_backtest(
    predictor: StockPredictor,
    feature_engineer: FeatureEngineer,
    data_dict: dict,
    initial_capital: float = 100000,
    start_date: Optional[datetime] = None,
    end_date: Optional[datetime] = None
) -> dict:
    """
    è¿è¡Œå›æµ‹
    
    Args:
        predictor: é¢„æµ‹æ¨¡å‹
        feature_engineer: ç‰¹å¾å·¥ç¨‹å™¨
        data_dict: æ•°æ®å­—å…¸
        initial_capital: åˆå§‹èµ„é‡‘
        start_date: å¼€å§‹æ—¥æœŸ
        end_date: ç»“æŸæ—¥æœŸ
        
    Returns:
        results: å›æµ‹ç»“æœ
    """
    print("\n" + "=" * 60)
    print("ğŸ”„ STEP 4: Running Backtest")
    print("=" * 60)
    
    # åˆ›å»ºå›æµ‹å¼•æ“
    engine = BacktestEngine(
        initial_capital=initial_capital,
        commission_rate=0.001,  # 0.1% ä½£é‡‘
        slippage_rate=0.001,    # 0.1% æ»‘ç‚¹
        market='US'
    )
    
    # åˆ›å»ºç­–ç•¥
    strategy = WeeklyRotationStrategy(
        predictor=predictor,
        feature_engineer=feature_engineer,
        top_k=1,  # æ¯æ¬¡åªé€‰1åªè‚¡ç¥¨
        rebalance_weekday=0  # å‘¨ä¸€
    )
    
    print(f"\nStrategy: {strategy.name}")
    print(f"Initial Capital: ${initial_capital:,.2f}")
    print(f"Top K: {strategy.top_k}")
    print(f"Rebalance: Every Monday")
    
    # è¿è¡Œå›æµ‹
    print("\nRunning backtest...")
    results = run_backtest_with_strategy(
        engine=engine,
        strategy=strategy,
        data=data_dict,
        start_date=start_date,
        end_date=end_date,
        verbose=True
    )
    
    print("\nâœ… Backtest completed!")
    
    return results


def analyze_performance(results: dict):
    """
    åˆ†ææ€§èƒ½
    
    Args:
        results: å›æµ‹ç»“æœ
    """
    print("\n" + "=" * 60)
    print("ğŸ“ˆ STEP 5: Performance Analysis")
    print("=" * 60)
    
    # åˆ›å»ºæ€§èƒ½åˆ†æå™¨
    analyzer = PerformanceAnalyzer(risk_free_rate=0.02)
    
    # åˆ†æ
    analysis = analyzer.analyze(
        portfolio_df=results['portfolio'],
        trades_df=results['trades']
    )
    
    # æ‰“å°æŠ¥å‘Š
    analyzer.print_report(analysis)
    
    # ä¿å­˜ç»“æœ
    output_dir = "output"
    os.makedirs(output_dir, exist_ok=True)
    
    # ä¿å­˜äº¤æ˜“è®°å½•
    if len(results['trades']) > 0:
        trades_path = os.path.join(output_dir, "trades.csv")
        results['trades'].to_csv(trades_path, index=False)
        print(f"\nğŸ’¾ Trades saved to {trades_path}")
    
    # ä¿å­˜æŠ•èµ„ç»„åˆå†å²
    portfolio_path = os.path.join(output_dir, "portfolio.csv")
    results['portfolio'].to_csv(portfolio_path, index=False)
    print(f"ğŸ’¾ Portfolio history saved to {portfolio_path}")
    
    return analysis


def main():
    """ä¸»å‡½æ•°"""
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='Mag7 Weekly Rotation Strategy')
    parser.add_argument('--model_type', type=str, default='random_forest',
                       choices=['random_forest', 'ridge', 'lasso', 'linear', 'gbdt'],
                       help='Model type')
    parser.add_argument('--loss_type', type=str, default='rank_mse',
                       choices=['mse', 'rank_mse', 'pairwise', 'listnet'],
                       help='Loss function type')
    parser.add_argument('--data_dir', type=str, default='data',
                       help='Data directory')
    parser.add_argument('--start_date', type=str, default=None,
                       help='Start date for data fetching (YYYY-MM-DD). If not provided, will use --years')
    parser.add_argument('--end_date', type=str, default=None,
                       help='End date for data fetching (YYYY-MM-DD). If not provided, will use current date')
    parser.add_argument('--years', type=int, default=1,
                       help='Years of historical data (used when --start_date is not provided)')
    parser.add_argument('--forward_days', type=int, default=5,
                       help='Forward prediction days')
    parser.add_argument('--initial_capital', type=float, default=100000,
                       help='Initial capital')
    parser.add_argument('--test_ratio', type=float, default=0.3,
                       help='Test set ratio')
    parser.add_argument('--save_model', action='store_true',
                       help='Save trained model')
    parser.add_argument('--save_dataset', action='store_true',
                       help='Save processed train/test dataset')
    parser.add_argument('--load_dataset', type=str, default=None,
                       help='Load processed dataset from path (skip data fetching and feature engineering)')
    parser.add_argument('--dataset_path', type=str, default='output/dataset.pkl',
                       help='Path to save/load dataset')
    
    args = parser.parse_args()
    
    print("\n" + "=" * 60)
    print("MAG7 WEEKLY ROTATION STRATEGY".center(60))
    print("=" * 60)
    print(f"\nConfiguration:")
    print(f"  Model Type: {args.model_type}")
    print(f"  Loss Type: {args.loss_type}")
    print(f"  Forward Days: {args.forward_days}")
    print(f"  Initial Capital: ${args.initial_capital:,.2f}")
    print(f"  Test Ratio: {args.test_ratio}")
    if args.start_date:
        print(f"  Start Date: {args.start_date}")
    if args.end_date:
        print(f"  End Date: {args.end_date}")
    if args.load_dataset:
        print(f"  Loading Dataset: {args.load_dataset}")
    if args.save_dataset:
        print(f"  Saving Dataset: {args.dataset_path}")
    
    try:
        # åˆ¤æ–­æ˜¯åŠ è½½æ•°æ®é›†è¿˜æ˜¯é‡æ–°ç”Ÿæˆ
        if args.load_dataset:
            # ä»æ–‡ä»¶åŠ è½½æ•°æ®é›†
            train_data, test_data, feature_engineer = load_dataset(args.load_dataset)
            # å¦‚æœéœ€è¦å›æµ‹ï¼Œè¿˜éœ€è¦è·å–åŸå§‹æ•°æ®
            data_dict = fetch_mag7_data(
                data_dir=args.data_dir,
                start_date=args.start_date,
                end_date=args.end_date,
                years=args.years
            )
        else:
            # 1. è·å–æ•°æ®
            data_dict = fetch_mag7_data(
                data_dir=args.data_dir,
                start_date=args.start_date,
                end_date=args.end_date,
                years=args.years
            )
            
            if len(data_dict) < 3:
                print("\nâŒ Error: Not enough data. Need at least 3 stocks.")
                return
            
            # 2. å‡†å¤‡è®­ç»ƒæ•°æ®
            train_data, test_data, feature_engineer = prepare_training_data(
                data_dict,
                forward_days=args.forward_days,
                test_ratio=args.test_ratio
            )
            
            # ä¿å­˜æ•°æ®é›†ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if args.save_dataset:
                save_dataset(
                    train_data,
                    test_data,
                    feature_engineer,
                    args.dataset_path
                )
        
        # 3. è®­ç»ƒæ¨¡å‹
        model_path = f"output/model_{args.model_type}_{args.loss_type}.pkl" if args.save_model else None
        predictor = train_model(
            train_data,
            test_data,
            model_type=args.model_type,
            loss_type=args.loss_type,
            save_path=model_path
        )
        
        # 4. è¿è¡Œå›æµ‹ï¼ˆä½¿ç”¨æµ‹è¯•é›†æ—¥æœŸèŒƒå›´ï¼‰
        _, _, dates_test = test_data
        start_date = pd.Timestamp(min(dates_test))
        end_date = pd.Timestamp(max(dates_test))
        
        results = run_backtest(
            predictor=predictor,
            feature_engineer=feature_engineer,
            data_dict=data_dict,
            initial_capital=args.initial_capital,
            start_date=start_date,
            end_date=end_date
        )
        
        # 5. æ€§èƒ½åˆ†æ
        analysis = analyze_performance(results)
        
        print("\n" + "=" * 60)
        print("âœ… ALL DONE!".center(60))
        print("=" * 60)
        
    except Exception as e:
        print(f"\nâŒ Error: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0


if __name__ == "__main__":
    sys.exit(main())

