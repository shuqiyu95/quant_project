"""
Aè‚¡æ¬¡æ—¥é«˜ç‚¹é¢„æµ‹ç­–ç•¥

é¢„æµ‹æ¬¡æ—¥å¼€ç›˜å30åˆ†é’Ÿå†…çš„æœ€é«˜æ¶¨å¹…ï¼Œç”¨äºå¼€ç›˜ä¹°å…¥ã€é«˜ç‚¹å–å‡ºçš„äº¤æ˜“ç­–ç•¥

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. æ ‡ç­¾ç”Ÿæˆï¼šè®¡ç®—æ¬¡æ—¥å¼€ç›˜å30åˆ†é’Ÿæœ€é«˜æ¶¨å¹…
2. Aè‚¡ç‰¹è‰²å› å­ï¼šç«ä»·å¼ºåº¦ã€é‡èƒ½ã€æ¿å—åŠ¨é‡ç­‰
3. å¤šåˆ†ç±»æ¨¡å‹ï¼šå°†æ¶¨å¹…åˆ†ä¸º5ä¸ªæ¡¶ (+6%, +3%, 0%, -3%, -6%)
4. å›æµ‹ï¼šè¯„ä¼°ç­–ç•¥æ”¶ç›Š

ä½œè€…ï¼šQuant Team
æ—¥æœŸï¼š2025-12-23
"""

import os
import sys
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional
import warnings
warnings.filterwarnings('ignore')

from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import joblib

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from src.data_engine import DataManager
from src.factors import Alpha158
from src.backtester import BacktestEngine


class CNIntradayHighPredictor:
    """
    Aè‚¡æ¬¡æ—¥é«˜ç‚¹é¢„æµ‹å™¨
    
    é¢„æµ‹æ¬¡æ—¥å¼€ç›˜å30åˆ†é’Ÿå†…çš„æœ€é«˜æ¶¨å¹…åŒºé—´
    """
    
    # æ¶¨å¹…æ¡¶å®šä¹‰ï¼ˆç™¾åˆ†æ¯”ï¼‰
    BINS = [-np.inf, -3.0, 0.0, 3.0, 6.0, np.inf]
    LABELS = ['<-3%', '-3%~0%', '0%~3%', '3%~6%', '>6%']
    
    def __init__(
        self,
        model_type: str = 'random_forest',
        model_params: Optional[Dict] = None,
        data_dir: str = 'data'
    ):
        """
        Args:
            model_type: æ¨¡å‹ç±»å‹ ('random_forest' or 'gbdt')
            model_params: æ¨¡å‹å‚æ•°
            data_dir: æ•°æ®ç›®å½•
        """
        self.model_type = model_type
        self.model_params = model_params or self._default_params()
        self.data_dir = data_dir
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.dm = DataManager(data_dir=data_dir)
        self.alpha158 = Alpha158()
        self.scaler = StandardScaler()
        
        # æ¨¡å‹
        self.model = self._create_model()
        
        # ç‰¹å¾ç›¸å…³
        self.feature_names_ = None
        self.feature_importance_ = None
        
        print(f"âœ… CNIntradayHighPredictor initialized ({model_type})")
    
    def _default_params(self) -> Dict:
        """é»˜è®¤æ¨¡å‹å‚æ•°"""
        if self.model_type == 'random_forest':
            return {
                'n_estimators': 200,
                'max_depth': 15,
                'min_samples_split': 20,
                'min_samples_leaf': 10,
                'class_weight': 'balanced',
                'random_state': 42,
                'n_jobs': -1
            }
        else:  # gbdt
            return {
                'n_estimators': 200,
                'max_depth': 8,
                'learning_rate': 0.05,
                'random_state': 42
            }
    
    def _create_model(self):
        """åˆ›å»ºåˆ†ç±»æ¨¡å‹"""
        if self.model_type == 'random_forest':
            return RandomForestClassifier(**self.model_params)
        elif self.model_type == 'gbdt':
            return GradientBoostingClassifier(**self.model_params)
        else:
            raise ValueError(f"Unknown model type: {self.model_type}")
    
    def generate_label(
        self,
        symbol: str,
        date: pd.Timestamp,
        intraday_df: pd.DataFrame
    ) -> Optional[float]:
        """
        ç”Ÿæˆæ ‡ç­¾ï¼šæ¬¡æ—¥å¼€ç›˜å30åˆ†é’Ÿæœ€é«˜æ¶¨å¹…
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            date: å½“å‰æ—¥æœŸ
            intraday_df: åˆ†é’Ÿçº¿æ•°æ®
            
        Returns:
            max_return: å¼€ç›˜å30åˆ†é’Ÿæœ€é«˜æ¶¨å¹…ï¼ˆç™¾åˆ†æ¯”ï¼‰
        """
        try:
            # æ‰¾åˆ°æ¬¡æ—¥çš„æ•°æ®
            next_day = date + timedelta(days=1)
            
            # ç­›é€‰æ¬¡æ—¥çš„æ•°æ®
            next_day_data = intraday_df[
                (intraday_df.index.date == next_day.date())
            ]
            
            if len(next_day_data) == 0:
                return None
            
            # è·å–å¼€ç›˜ä»·ï¼ˆ9:30ï¼‰
            open_price = next_day_data.iloc[0]['open']
            
            # è·å–å¼€ç›˜å30åˆ†é’Ÿå†…çš„æ•°æ®ï¼ˆ9:30-10:00ï¼‰
            morning_data = next_day_data.between_time('09:30', '10:00')
            
            if len(morning_data) == 0:
                return None
            
            # è®¡ç®—30åˆ†é’Ÿå†…çš„æœ€é«˜æ¶¨å¹…
            high_price = morning_data['high'].max()
            max_return = (high_price - open_price) / open_price * 100  # è½¬æ¢ä¸ºç™¾åˆ†æ¯”
            
            return max_return
            
        except Exception as e:
            print(f"Warning: Failed to generate label for {symbol} on {date}: {e}")
            return None
    
    def calculate_cn_special_factors(
        self,
        daily_df: pd.DataFrame,
        intraday_df: Optional[pd.DataFrame] = None,
        date: Optional[pd.Timestamp] = None
    ) -> pd.DataFrame:
        """
        è®¡ç®—Aè‚¡ç‰¹è‰²å› å­
        
        åŒ…æ‹¬ï¼š
        1. ç«ä»·å¼ºåº¦ï¼šç«ä»·é‡å å…¨å¤©æ¯”ä¾‹ã€ç«ä»·æ¶¨å¹…
        2. é‡èƒ½å› å­ï¼šæ¢æ‰‹ç‡åˆ†ä½æ•°ã€é‡æ¯”
        3. æ¿å—åŠ¨é‡ï¼šè¿‘æœŸæ¶¨è·Œå¹…ã€æŒ¯å¹…
        4. æƒ…ç»ªå› å­ï¼šè¿ç»­æ¶¨è·Œå¤©æ•°ã€æ˜¯å¦æ¶¨åœ/è·Œåœ
        
        Args:
            daily_df: æ—¥çº¿æ•°æ®
            intraday_df: åˆ†é’Ÿçº¿æ•°æ®ï¼ˆå¯é€‰ï¼Œç”¨äºç«ä»·æ•°æ®ï¼‰
            date: å½“å‰æ—¥æœŸï¼ˆå¯é€‰ï¼Œç”¨äºè®¡ç®—å®æ—¶å› å­ï¼‰
            
        Returns:
            factors_df: å› å­DataFrame
        """
        factors = pd.DataFrame(index=daily_df.index)
        
        # ========== 1. åŸºç¡€ä»·æ ¼å› å­ ==========
        close = daily_df['close']
        high = daily_df['high']
        low = daily_df['low']
        volume = daily_df['volume']
        
        # æ¶¨è·Œå¹…åºåˆ—
        returns = close.pct_change()
        
        # ========== 2. é‡èƒ½å› å­ ==========
        if 'turnover' in daily_df.columns:
            # æ¢æ‰‹ç‡åˆ†ä½æ•°ï¼ˆè¿‘100å¤©ï¼‰
            factors['turnover_quantile'] = daily_df['turnover'].rolling(100).apply(
                lambda x: (x.iloc[-1] > x).sum() / len(x) if len(x) > 0 else 0.5
            )
            factors['turnover_ma5'] = daily_df['turnover'].rolling(5).mean()
            factors['turnover_ma20'] = daily_df['turnover'].rolling(20).mean()
        else:
            # ç”¨æˆäº¤é‡ä»£æ›¿
            factors['turnover_quantile'] = volume.rolling(100).apply(
                lambda x: (x.iloc[-1] > x).sum() / len(x) if len(x) > 0 else 0.5
            )
        
        # é‡æ¯”ï¼ˆä»Šæ—¥é‡/5æ—¥å‡é‡ï¼‰
        volume_ma5 = volume.rolling(5).mean()
        factors['volume_ratio'] = volume / (volume_ma5 + 1e-10)
        
        # æˆäº¤é‡å˜åŒ–ç‡
        factors['volume_change_5d'] = volume.pct_change(5)
        factors['volume_change_20d'] = volume.pct_change(20)
        
        # ========== 3. ä»·æ ¼åŠ¨é‡å› å­ ==========
        # å¤šå‘¨æœŸæ”¶ç›Šç‡
        for d in [1, 3, 5, 10, 20]:
            factors[f'return_{d}d'] = returns.rolling(d).sum()
        
        # æŒ¯å¹…
        for d in [5, 10, 20]:
            factors[f'amplitude_{d}d'] = ((high - low) / close).rolling(d).mean()
        
        # ä»·æ ¼å¼ºåº¦ï¼ˆæ”¶ç›˜ä»·åœ¨å½“æ—¥èŒƒå›´å†…çš„ä½ç½®ï¼‰
        factors['price_position'] = (close - low) / (high - low + 1e-10)
        factors['price_position_ma5'] = factors['price_position'].rolling(5).mean()
        
        # ========== 4. æƒ…ç»ªå› å­ ==========
        # è¿ç»­æ¶¨è·Œå¤©æ•°
        factors['consecutive_up'] = (returns > 0).astype(int).groupby(
            (returns <= 0).astype(int).cumsum()
        ).cumsum()
        factors['consecutive_down'] = (returns < 0).astype(int).groupby(
            (returns >= 0).astype(int).cumsum()
        ).cumsum()
        
        # è¿‘æœŸåˆ›æ–°é«˜/æ–°ä½
        factors['is_high_20d'] = (close == close.rolling(20).max()).astype(int)
        factors['is_low_20d'] = (close == close.rolling(20).min()).astype(int)
        
        # ========== 5. æ³¢åŠ¨ç‡å› å­ ==========
        # å†å²æ³¢åŠ¨ç‡
        for d in [5, 10, 20]:
            factors[f'volatility_{d}d'] = returns.rolling(d).std() * np.sqrt(252)
        
        # ä¸Šè¡Œ/ä¸‹è¡Œæ³¢åŠ¨ç‡
        upside_vol = returns[returns > 0].rolling(20).std()
        downside_vol = returns[returns < 0].rolling(20).std()
        factors['upside_volatility'] = upside_vol.fillna(0)
        factors['downside_volatility'] = downside_vol.fillna(0)
        
        # ========== 6. ç«ä»·å› å­ï¼ˆéœ€è¦åˆ†é’Ÿçº¿æ•°æ®ï¼‰==========
        if intraday_df is not None and len(intraday_df) > 0:
            # å¯¹æ¯ä¸ªæ—¥æœŸè®¡ç®—ç«ä»·å› å­
            for idx in daily_df.index:
                try:
                    day_minute_data = intraday_df[
                        intraday_df.index.date == idx.date()
                    ]
                    
                    if len(day_minute_data) > 0:
                        # ç«ä»·é‡ï¼ˆ9:25-9:30ï¼‰
                        auction_data = day_minute_data.between_time('09:25', '09:30')
                        if len(auction_data) > 0:
                            auction_volume = auction_data['volume'].sum()
                            day_total_volume = day_minute_data['volume'].sum()
                            factors.loc[idx, 'auction_volume_ratio'] = auction_volume / (day_total_volume + 1e-10)
                            
                            # ç«ä»·æ¶¨å¹…
                            if idx in daily_df.index:
                                prev_close = daily_df.loc[:idx]['close'].iloc[-2] if len(daily_df.loc[:idx]) > 1 else None
                                if prev_close is not None:
                                    auction_price = auction_data.iloc[0]['open']
                                    factors.loc[idx, 'auction_return'] = (auction_price - prev_close) / prev_close
                except Exception:
                    continue
        
        # å¡«å……ç¼ºå¤±å€¼
        factors = factors.fillna(0)
        
        return factors
    
    def prepare_features(
        self,
        daily_df: pd.DataFrame,
        intraday_df: Optional[pd.DataFrame] = None
    ) -> pd.DataFrame:
        """
        å‡†å¤‡å®Œæ•´ç‰¹å¾é›†
        
        ç»“åˆ Alpha158 å› å­ + Aè‚¡ç‰¹è‰²å› å­
        
        Args:
            daily_df: æ—¥çº¿æ•°æ®
            intraday_df: åˆ†é’Ÿçº¿æ•°æ®ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            features_df: å®Œæ•´ç‰¹å¾DataFrame
        """
        # 1. è®¡ç®— Alpha158 å› å­
        alpha158_factors = self.alpha158.calculate(daily_df)
        
        # 2. è®¡ç®— Aè‚¡ç‰¹è‰²å› å­
        cn_factors = self.calculate_cn_special_factors(daily_df, intraday_df)
        
        # 3. åˆå¹¶
        features = pd.concat([alpha158_factors, cn_factors], axis=1)
        
        # 4. ç§»é™¤æ— æ•ˆè¡Œï¼ˆå‰é¢å› æ»šåŠ¨çª—å£äº§ç”Ÿçš„NaNï¼‰
        features = features.replace([np.inf, -np.inf], np.nan)
        features = features.dropna()
        
        return features
    
    def prepare_dataset(
        self,
        symbol: str,
        start_date: str,
        end_date: str,
        use_cache: bool = True,
        min_periods: int = 60
    ) -> Tuple[pd.DataFrame, pd.Series, List]:
        """
        å‡†å¤‡è®­ç»ƒæ•°æ®é›†
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
            min_periods: æœ€å°å‘¨æœŸæ•°ï¼ˆç”¨äºè®¡ç®—å› å­ï¼‰
            
        Returns:
            X: ç‰¹å¾çŸ©é˜µ
            y: æ ‡ç­¾ï¼ˆæ¶¨å¹…æ¡¶ï¼‰
            dates: æ—¥æœŸåˆ—è¡¨
        """
        print(f"\n{'='*60}")
        print(f"ğŸ“Š Preparing dataset for {symbol}")
        print(f"{'='*60}")
        
        # 1. è·å–æ—¥çº¿æ•°æ®ï¼ˆæ‰©å±•æ—¶é—´èŒƒå›´ä»¥ç¡®ä¿æœ‰è¶³å¤Ÿçš„å†å²æ•°æ®ï¼‰
        extended_start = (datetime.strptime(start_date, '%Y-%m-%d') - timedelta(days=365)).strftime('%Y-%m-%d')
        
        print(f"\nğŸ“¥ Fetching daily data...")
        daily_df = self.dm.fetch_data(
            symbol=symbol,
            start_date=extended_start,
            end_date=end_date,
            use_cache=use_cache
        )
        
        if daily_df is None or len(daily_df) < min_periods:
            raise ValueError(f"Insufficient daily data for {symbol}")
        
        print(f"âœ… Daily data: {len(daily_df)} days")
        
        # 2. è·å–åˆ†é’Ÿçº¿æ•°æ®ï¼ˆç”¨äºç”Ÿæˆæ ‡ç­¾å’Œç«ä»·å› å­ï¼‰
        print(f"\nğŸ“¥ Fetching minute data...")
        try:
            minute_df = self.dm.cn_fetcher.fetch_intraday_data(
                symbol=symbol,
                start_date=datetime.strptime(start_date, '%Y-%m-%d'),
                end_date=datetime.strptime(end_date, '%Y-%m-%d'),
                period="1"
            )
            print(f"âœ… Minute data: {len(minute_df)} bars")
        except Exception as e:
            print(f"âš ï¸ Warning: Failed to fetch minute data: {e}")
            minute_df = pd.DataFrame()
        
        # 3. è®¡ç®—ç‰¹å¾
        print(f"\nğŸ”§ Calculating features...")
        features = self.prepare_features(daily_df, minute_df if len(minute_df) > 0 else None)
        print(f"âœ… Features: {features.shape[1]} columns")
        
        # 4. ç”Ÿæˆæ ‡ç­¾
        print(f"\nğŸ·ï¸  Generating labels...")
        labels = []
        valid_dates = []
        
        for date in features.index:
            # åªå¤„ç†ç›®æ ‡æ—¶é—´èŒƒå›´å†…çš„æ•°æ®
            if date < pd.Timestamp(start_date, tz=date.tz):
                continue
            
            if len(minute_df) > 0:
                label = self.generate_label(symbol, date, minute_df)
            else:
                # å¦‚æœæ²¡æœ‰åˆ†é’Ÿçº¿æ•°æ®ï¼Œç”¨æ¬¡æ—¥å¼€ç›˜ä»·ä½œä¸ºè¿‘ä¼¼
                try:
                    next_day_idx = daily_df.index.get_loc(date) + 1
                    if next_day_idx < len(daily_df):
                        next_open = daily_df.iloc[next_day_idx]['open']
                        curr_close = daily_df.loc[date, 'close']
                        label = (next_open - curr_close) / curr_close * 100
                    else:
                        label = None
                except Exception:
                    label = None
            
            if label is not None:
                labels.append(label)
                valid_dates.append(date)
        
        print(f"âœ… Labels: {len(labels)} samples")
        
        # 5. å¯¹é½ç‰¹å¾å’Œæ ‡ç­¾
        features = features.loc[valid_dates]
        
        # 6. å°†è¿ç»­æ ‡ç­¾è½¬æ¢ä¸ºåˆ†ç±»æ¡¶
        y_continuous = pd.Series(labels, index=valid_dates)
        y_categorical = pd.cut(
            y_continuous,
            bins=self.BINS,
            labels=range(len(self.LABELS))
        ).astype(int)
        
        # 7. ä¿å­˜ç‰¹å¾åç§°
        self.feature_names_ = features.columns.tolist()
        
        # æ‰“å°æ ‡ç­¾åˆ†å¸ƒ
        print(f"\nğŸ“Š Label distribution:")
        for i, label in enumerate(self.LABELS):
            count = (y_categorical == i).sum()
            pct = count / len(y_categorical) * 100
            print(f"   {label}: {count} ({pct:.1f}%)")
        
        return features, y_categorical, valid_dates
    
    def train(
        self,
        X: pd.DataFrame,
        y: pd.Series,
        validation_split: float = 0.2
    ):
        """
        è®­ç»ƒæ¨¡å‹
        
        Args:
            X: ç‰¹å¾çŸ©é˜µ
            y: æ ‡ç­¾ï¼ˆåˆ†ç±»æ¡¶ï¼‰
            validation_split: éªŒè¯é›†æ¯”ä¾‹
        """
        print(f"\n{'='*60}")
        print(f"ğŸ¤– Training model")
        print(f"{'='*60}")
        
        # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†ï¼ˆæŒ‰æ—¶é—´é¡ºåºï¼‰
        split_idx = int(len(X) * (1 - validation_split))
        X_train, X_val = X.iloc[:split_idx], X.iloc[split_idx:]
        y_train, y_val = y.iloc[:split_idx], y.iloc[split_idx:]
        
        print(f"\nğŸ“Š Dataset split:")
        print(f"   Training: {len(X_train)} samples")
        print(f"   Validation: {len(X_val)} samples")
        
        # ç‰¹å¾æ ‡å‡†åŒ–
        print(f"\nâš™ï¸  Scaling features...")
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_val_scaled = self.scaler.transform(X_val)
        
        # è®­ç»ƒæ¨¡å‹
        print(f"\nğŸ¯ Training {self.model_type}...")
        self.model.fit(X_train_scaled, y_train)
        
        # è¯„ä¼°
        print(f"\nğŸ“ˆ Evaluation:")
        
        # è®­ç»ƒé›†
        y_train_pred = self.model.predict(X_train_scaled)
        train_acc = accuracy_score(y_train, y_train_pred)
        print(f"   Training Accuracy: {train_acc:.4f}")
        
        # éªŒè¯é›†
        y_val_pred = self.model.predict(X_val_scaled)
        val_acc = accuracy_score(y_val, y_val_pred)
        print(f"   Validation Accuracy: {val_acc:.4f}")
        
        # è¯¦ç»†æŠ¥å‘Š
        print(f"\nğŸ“Š Classification Report (Validation):")
        # åªæ˜¾ç¤ºå®é™…å­˜åœ¨çš„ç±»åˆ«
        unique_classes = sorted(y_val.unique())
        class_labels = [self.LABELS[i] for i in unique_classes]
        print(classification_report(
            y_val,
            y_val_pred,
            labels=unique_classes,
            target_names=class_labels,
            zero_division=0
        ))
        
        # ç‰¹å¾é‡è¦æ€§
        if hasattr(self.model, 'feature_importances_'):
            self.feature_importance_ = self.model.feature_importances_
            
            # æ‰“å° Top 20 ç‰¹å¾
            importance_df = pd.DataFrame({
                'feature': self.feature_names_,
                'importance': self.feature_importance_
            }).sort_values('importance', ascending=False)
            
            print(f"\nğŸ” Top 20 Important Features:")
            print(importance_df.head(20).to_string(index=False))
        
        print(f"\nâœ… Training completed!")
    
    def predict(self, X: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:
        """
        é¢„æµ‹
        
        Args:
            X: ç‰¹å¾çŸ©é˜µ
            
        Returns:
            y_pred: é¢„æµ‹ç±»åˆ«
            y_proba: é¢„æµ‹æ¦‚ç‡
        """
        X_scaled = self.scaler.transform(X)
        y_pred = self.model.predict(X_scaled)
        y_proba = self.model.predict_proba(X_scaled)
        return y_pred, y_proba
    
    def backtest(
        self,
        symbol: str,
        X_test: pd.DataFrame,
        dates_test: List,
        daily_df: pd.DataFrame,
        initial_capital: float = 100000.0
    ) -> Dict:
        """
        å›æµ‹ç­–ç•¥
        
        ç­–ç•¥ï¼šæ¯å¤©é¢„æµ‹æ¬¡æ—¥é«˜ç‚¹ï¼Œå¦‚æœé¢„æµ‹æ¶¨å¹… > 3%ï¼Œåˆ™å¼€ç›˜ä¹°å…¥ï¼Œ30åˆ†é’Ÿåå–å‡º
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            X_test: æµ‹è¯•é›†ç‰¹å¾
            dates_test: æµ‹è¯•é›†æ—¥æœŸ
            daily_df: æ—¥çº¿æ•°æ®ï¼ˆç”¨äºè·å–ä»·æ ¼ï¼‰
            initial_capital: åˆå§‹èµ„é‡‘
            
        Returns:
            backtest_results: å›æµ‹ç»“æœå­—å…¸
        """
        print(f"\n{'='*60}")
        print(f"ğŸ“Š Backtesting strategy")
        print(f"{'='*60}")
        
        # é¢„æµ‹
        y_pred, y_proba = self.predict(X_test)
        
        # å›æµ‹å¼•æ“
        engine = BacktestEngine(
            initial_capital=initial_capital,
            commission_rate=0.0003,  # Aè‚¡ä½£é‡‘ 0.03%
            slippage_rate=0.001,     # æ»‘ç‚¹ 0.1%
            market='CN'
        )
        
        # æ¨¡æ‹Ÿäº¤æ˜“
        trades = []
        positions = []
        
        for i, date in enumerate(dates_test):
            pred_class = y_pred[i]
            pred_proba_max = y_proba[i].max()
            
            # ç­–ç•¥ï¼šå¦‚æœé¢„æµ‹æ¶¨å¹… >= 3% (class >= 3) ä¸”ç½®ä¿¡åº¦ > 0.4ï¼Œåˆ™äº¤æ˜“
            if pred_class >= 3 and pred_proba_max > 0.4:
                # è·å–æ¬¡æ—¥å¼€ç›˜ä»·
                try:
                    next_day_idx = daily_df.index.get_loc(date) + 1
                    if next_day_idx >= len(daily_df):
                        continue
                    
                    next_day_date = daily_df.index[next_day_idx]
                    open_price = daily_df.iloc[next_day_idx]['open']
                    
                    # å‡è®¾30åˆ†é’Ÿåå–å‡ºï¼ˆç®€åŒ–ç‰ˆï¼šç”¨å½“æ—¥é«˜ç‚¹çš„ä¸€éƒ¨åˆ†ä½œä¸ºå–å‡ºä»·ï¼‰
                    # å®é™…åº”è¯¥ç”¨åˆ†é’Ÿçº¿æ•°æ®
                    high_price = daily_df.iloc[next_day_idx]['high']
                    sell_price = open_price + (high_price - open_price) * 0.5  # ä¿å®ˆä¼°è®¡
                    
                    # è®¡ç®—æ”¶ç›Š
                    actual_return = (sell_price - open_price) / open_price
                    
                    # å…¨ä»“ä¹°å…¥
                    buy_value = engine.cash * 0.95  # ç•™ä¸€ç‚¹ä½™é‡
                    
                    # ä¹°å…¥
                    success = engine.buy(symbol, open_price, buy_value, next_day_date)
                    
                    if success:
                        # å–å‡º
                        engine.sell(symbol, sell_price, next_day_date)
                        
                        trades.append({
                            'date': date,
                            'trade_date': next_day_date,
                            'pred_class': pred_class,
                            'pred_label': self.LABELS[pred_class],
                            'confidence': pred_proba_max,
                            'open_price': open_price,
                            'sell_price': sell_price,
                            'return': actual_return * 100
                        })
                
                except Exception as e:
                    continue
            
            # æ›´æ–°æŠ•èµ„ç»„åˆ
            if date in daily_df.index:
                prices = {symbol: daily_df.loc[date, 'close']}
                engine.update_portfolio(date, prices)
        
        # ç»Ÿè®¡ç»“æœ
        print(f"\nğŸ“Š Backtest Results:")
        print(f"   Total trades: {len(trades)}")
        
        if len(trades) > 0:
            trades_df = pd.DataFrame(trades)
            avg_return = trades_df['return'].mean()
            win_rate = (trades_df['return'] > 0).sum() / len(trades_df)
            
            print(f"   Average return per trade: {avg_return:.2f}%")
            print(f"   Win rate: {win_rate:.2%}")
            print(f"\n   Recent trades:")
            print(trades_df.tail(10).to_string(index=False))
        
        # è·å–æŠ•èµ„ç»„åˆç»Ÿè®¡
        stats = engine.get_portfolio_stats()
        
        print(f"\nğŸ’° Portfolio Performance:")
        print(f"   Initial capital: Â¥{stats['initial_capital']:,.2f}")
        print(f"   Final value: Â¥{stats['final_value']:,.2f}")
        print(f"   Total return: {stats['total_return']:.2%}")
        print(f"   Annual return: {stats['annual_return']:.2%}")
        print(f"   Sharpe ratio: {stats['sharpe_ratio']:.4f}")
        print(f"   Max drawdown: {stats['max_drawdown']:.2%}")
        
        return {
            'trades': trades_df if len(trades) > 0 else pd.DataFrame(),
            'portfolio_stats': stats,
            'engine': engine
        }
    
    def save(self, filepath: str):
        """ä¿å­˜æ¨¡å‹"""
        model_data = {
            'model': self.model,
            'scaler': self.scaler,
            'model_type': self.model_type,
            'feature_names': self.feature_names_,
            'feature_importance': self.feature_importance_,
            'bins': self.BINS,
            'labels': self.LABELS
        }
        joblib.dump(model_data, filepath)
        print(f"\nâœ… Model saved to {filepath}")
    
    def load(self, filepath: str):
        """åŠ è½½æ¨¡å‹"""
        model_data = joblib.load(filepath)
        self.model = model_data['model']
        self.scaler = model_data['scaler']
        self.model_type = model_data['model_type']
        self.feature_names_ = model_data['feature_names']
        self.feature_importance_ = model_data.get('feature_importance')
        print(f"\nâœ… Model loaded from {filepath}")


if __name__ == "__main__":
    print("=" * 60)
    print("Aè‚¡æ¬¡æ—¥é«˜ç‚¹é¢„æµ‹ç­–ç•¥")
    print("=" * 60)
    print("\nä½¿ç”¨æ–¹æ³•:")
    print("  from cn_intraday_high_strategy import CNIntradayHighPredictor")
    print("  predictor = CNIntradayHighPredictor()")
    print("  X, y, dates = predictor.prepare_dataset('000858', '2023-01-01', '2024-12-31')")
    print("  predictor.train(X, y)")
    print("\nè¯¦è§ demo_cn_intraday_high.py")

