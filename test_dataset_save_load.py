"""
ÊµãËØïÊï∞ÊçÆÈõÜ‰øùÂ≠òÂíåÂä†ËΩΩÂäüËÉΩ

Usage:
    python test_dataset_save_load.py
"""

import os
import sys
from datetime import datetime, timedelta

# Ê∑ªÂä†È°πÁõÆË∑ØÂæÑ
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from src.data_engine import DataManager
from src.models import FeatureEngineer

# Test configuration
MAG7_SYMBOLS = ['AAPL', 'MSFT', 'GOOGL']  # ‰ΩøÁî®3‰∏™ËÇ°Á•®ËøõË°åÂø´ÈÄüÊµãËØï
TEST_DATASET_PATH = "output/test_dataset.pkl"


def test_dataset_save_load():
    """ÊµãËØïÊï∞ÊçÆÈõÜÁöÑ‰øùÂ≠òÂíåÂä†ËΩΩ"""
    
    print("\n" + "=" * 60)
    print("ÊµãËØïÊï∞ÊçÆÈõÜ‰øùÂ≠òÂíåÂä†ËΩΩÂäüËÉΩ")
    print("=" * 60)
    
    # Step 1: Ëé∑ÂèñÊµãËØïÊï∞ÊçÆ
    print("\nüìä Step 1: Ëé∑ÂèñÊµãËØïÊï∞ÊçÆ...")
    dm = DataManager(data_dir="data")
    
    end_date = datetime.now()
    start_date = end_date - timedelta(days=180)  # 6‰∏™ÊúàÊï∞ÊçÆÁî®‰∫éÊµãËØï
    
    data_dict = {}
    for symbol in MAG7_SYMBOLS:
        try:
            df = dm.fetch_data(
                symbol=symbol,
                start_date=start_date.strftime('%Y-%m-%d'),
                end_date=end_date.strftime('%Y-%m-%d'),
                use_cache=True
            )
            if df is not None and len(df) > 0:
                data_dict[symbol] = df
                print(f"‚úÖ {symbol}: {len(df)} days")
        except Exception as e:
            print(f"‚ùå {symbol}: Error - {e}")
    
    if len(data_dict) < 2:
        print("‚ùå ÊµãËØïÂ§±Ë¥•ÔºöÊï∞ÊçÆ‰∏çË∂≥")
        return False
    
    # Step 2: ÁâπÂæÅÂ∑•Á®ã
    print("\nüîß Step 2: ÁâπÂæÅÂ∑•Á®ã...")
    fe = FeatureEngineer()
    
    X, y, dates, symbols = fe.prepare_dataset(
        data_dict,
        forward_days=5,
        min_periods=30
    )
    
    print(f"‚úÖ ÁâπÂæÅ: {X.shape}, Ê†áÁ≠æ: {y.shape}")
    
    # ÂàíÂàÜËÆ≠ÁªÉÊµãËØïÈõÜ
    n_samples = len(X)
    split_idx = int(n_samples * 0.7)
    
    X_train = X.iloc[:split_idx]
    X_test = X.iloc[split_idx:]
    y_train = y.iloc[:split_idx]
    y_test = y.iloc[split_idx:]
    dates_train = dates[:split_idx]
    dates_test = dates[split_idx:]
    
    train_data = (X_train, y_train, dates_train)
    test_data = (X_test, y_test, dates_test)
    
    # Step 3: ‰øùÂ≠òÊï∞ÊçÆÈõÜ
    print("\nüíæ Step 3: ‰øùÂ≠òÊï∞ÊçÆÈõÜ...")
    import pickle
    
    dataset = {
        'X_train': X_train,
        'y_train': y_train,
        'dates_train': dates_train,
        'X_test': X_test,
        'y_test': y_test,
        'dates_test': dates_test,
        'feature_engineer': fe,
        'metadata': {
            'train_samples': len(X_train),
            'test_samples': len(X_test),
            'features': X_train.shape[1],
            'train_date_range': (min(dates_train).date(), max(dates_train).date()),
            'test_date_range': (min(dates_test).date(), max(dates_test).date()),
            'saved_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }
    }
    
    os.makedirs(os.path.dirname(TEST_DATASET_PATH), exist_ok=True)
    
    with open(TEST_DATASET_PATH, 'wb') as f:
        pickle.dump(dataset, f)
    
    file_size = os.path.getsize(TEST_DATASET_PATH) / 1024 / 1024  # MB
    print(f"‚úÖ Êï∞ÊçÆÈõÜÂ∑≤‰øùÂ≠òÂà∞: {TEST_DATASET_PATH}")
    print(f"   Êñá‰ª∂Â§ßÂ∞è: {file_size:.2f} MB")
    print(f"   ËÆ≠ÁªÉÊ†∑Êú¨: {dataset['metadata']['train_samples']}")
    print(f"   ÊµãËØïÊ†∑Êú¨: {dataset['metadata']['test_samples']}")
    
    # Step 4: Âä†ËΩΩÊï∞ÊçÆÈõÜ
    print("\nüìÇ Step 4: Âä†ËΩΩÊï∞ÊçÆÈõÜ...")
    
    with open(TEST_DATASET_PATH, 'rb') as f:
        loaded_dataset = pickle.load(f)
    
    print(f"‚úÖ Êï∞ÊçÆÈõÜÂ∑≤Âä†ËΩΩ")
    print(f"   ‰øùÂ≠òÊó∂Èó¥: {loaded_dataset['metadata']['saved_at']}")
    print(f"   ËÆ≠ÁªÉÊ†∑Êú¨: {loaded_dataset['metadata']['train_samples']}")
    print(f"   ÊµãËØïÊ†∑Êú¨: {loaded_dataset['metadata']['test_samples']}")
    
    # Step 5: È™åËØÅÊï∞ÊçÆ‰∏ÄËá¥ÊÄß
    print("\nüîç Step 5: È™åËØÅÊï∞ÊçÆ‰∏ÄËá¥ÊÄß...")
    
    checks = [
        ("ËÆ≠ÁªÉÁâπÂæÅ", X_train.shape == loaded_dataset['X_train'].shape),
        ("ËÆ≠ÁªÉÊ†áÁ≠æ", y_train.shape == loaded_dataset['y_train'].shape),
        ("ÊµãËØïÁâπÂæÅ", X_test.shape == loaded_dataset['X_test'].shape),
        ("ÊµãËØïÊ†áÁ≠æ", y_test.shape == loaded_dataset['y_test'].shape),
        ("ËÆ≠ÁªÉÊó•Êúü", len(dates_train) == len(loaded_dataset['dates_train'])),
        ("ÊµãËØïÊó•Êúü", len(dates_test) == len(loaded_dataset['dates_test'])),
        ("ÁâπÂæÅÂ∑•Á®ãÂô®", loaded_dataset['feature_engineer'] is not None),
    ]
    
    all_passed = True
    for name, passed in checks:
        status = "‚úÖ" if passed else "‚ùå"
        print(f"{status} {name}")
        if not passed:
            all_passed = False
    
    # Step 6: Ê∏ÖÁêÜÊµãËØïÊñá‰ª∂
    print("\nüßπ Step 6: Ê∏ÖÁêÜÊµãËØïÊñá‰ª∂...")
    if os.path.exists(TEST_DATASET_PATH):
        os.remove(TEST_DATASET_PATH)
        print(f"‚úÖ Â∑≤Âà†Èô§ÊµãËØïÊñá‰ª∂: {TEST_DATASET_PATH}")
    
    # ÊúÄÁªàÁªìÊûú
    print("\n" + "=" * 60)
    if all_passed:
        print("‚úÖ ÊâÄÊúâÊµãËØïÈÄöËøáÔºÅ")
        print("=" * 60)
        return True
    else:
        print("‚ùå ÈÉ®ÂàÜÊµãËØïÂ§±Ë¥•")
        print("=" * 60)
        return False


if __name__ == "__main__":
    try:
        success = test_dataset_save_load()
        sys.exit(0 if success else 1)
    except Exception as e:
        print(f"\n‚ùå ÊµãËØïÂá∫Èîô: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

